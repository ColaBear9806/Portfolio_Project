{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#실행전 준비"
      ],
      "metadata": {
        "id": "GaLUw4Nb8oJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 출처: https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=580"
      ],
      "metadata": {
        "id": "ajqCaoHp8qK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "github ipynb 출력 오류로 셀 출력 없는 버전이 업로드 되었습니다.\n",
        "\n",
        "셀 출력 있는 버전은 colab에 업로드 되어있습니다.\n",
        "\n",
        "링크: https://colab.research.google.com/drive/1_NN0nExSkmpgAt7DWya3XZuSFa3uIGff?usp=sharing"
      ],
      "metadata": {
        "id": "k5i8-ZtSvngy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDyR63OTNUJ6"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "\n",
        "!pip install -q -U transformers bitsandbytes protobuf\n",
        "!pip install -q -U peft trl matplotlib wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hxhjjxQJvRBC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "src = list(files.upload().values())[0]\n",
        "open('make_data2.py','wb').write(src)"
      ],
      "metadata": {
        "id": "YGSWtBcSWd_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#라이브러리 import"
      ],
      "metadata": {
        "id": "jPqyOtAV82IH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yikV8pRBer9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoModelForImageTextToText, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import wandb\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from make_data2 import Datasetup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#데이터 로드"
      ],
      "metadata": {
        "id": "cF8zcgnM855d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/law_ai/new_pickle/train.pkl\",\"rb\") as fr:\n",
        "    train_data = pickle.load(fr)"
      ],
      "metadata": {
        "id": "p7t3qUujfxCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/law_ai/new_pickle/test.pkl\",\"rb\") as fr:\n",
        "    test_data = pickle.load(fr)"
      ],
      "metadata": {
        "id": "OS02wSJ5f6mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/law_ai/new_pickle/val.pkl\",\"rb\") as fr:\n",
        "    val_data = pickle.load(fr)"
      ],
      "metadata": {
        "id": "mJYra2bLf8_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[1]"
      ],
      "metadata": {
        "id": "GjzlleyGedcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))\n",
        "print(len(test_data))\n",
        "print(len(val_data))"
      ],
      "metadata": {
        "id": "SXdsJWodfKH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#모델 학습 준비"
      ],
      "metadata": {
        "id": "DfG4oidD896Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuTX-xonNeOK"
      },
      "outputs": [],
      "source": [
        "#모델 설정\n",
        "MODEL_ID = \"google/gemma-3-4b-pt\"\n",
        "TOKENIZER_MODEL = \"google/gemma-3-4b-it\"\n",
        "PROJECT_NAME = \"law_ai5\"\n",
        "HF_USER = \"dlddu123\"\n",
        "\n",
        "#모델 불러오는 라이브러리 선택\n",
        "if MODEL_ID == \"google/gemma-3-1b-pt\":\n",
        "    MODEL_CLASS = AutoModelForCausalLM\n",
        "else:\n",
        "    MODEL_CLASS = AutoModelForImageTextToText\n",
        "\n",
        "#모델의 dtype 설정\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    torch_dtype = torch.bfloat16\n",
        "else:\n",
        "    torch_dtype = torch.float16\n",
        "\n",
        "#프로젝트 이름 설정\n",
        "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "#LoRA 하이퍼파라미터 설정\n",
        "peft_config = LoraConfig(\n",
        "    r = 32,\n",
        "    lora_alpha  = 32,\n",
        "    target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
        "    lora_dropout  = 0.1,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "#WANDB 설정\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "LEARNING_RATE = 1e-4\n",
        "LR_SCHEDULER_TYPE = 'cosine'\n",
        "WARMUP_RATIO = 0.03\n",
        "OPTIMIZER = \"adamw_torch_fused\"\n",
        "\n",
        "#WANDB 설정\n",
        "LOG_TO_WANDB = True\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyHOj-c4FmkM"
      },
      "outputs": [],
      "source": [
        "HUB_MODEL_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyFPZeMcM88v"
      },
      "outputs": [],
      "source": [
        "# Log in to HuggingFace\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WANDB 준비 및 시작"
      ],
      "metadata": {
        "id": "VySgT1e69WND"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJNOv3cVvJ68"
      },
      "outputs": [],
      "source": [
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key#from google.colab import userdata으로 colab에 저장된 키 가져오기\n",
        "\n",
        "#WANDB 프로젝트 설정\n",
        "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"gradients\"\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#혹시나 wandb가 이미 실행되어있을경우 초기화\n",
        "try:\n",
        "    if wandb.run is not None:\n",
        "        wandb.finish()\n",
        "except Exception:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "OZwUFh4bhmrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb_api_key = userdata.get('WANDB_API_KEY')  # 주피터/코랩 비밀변수 등에서 가져오는 경우\n",
        "if not wandb_api_key or not str(wandb_api_key).strip():\n",
        "    raise ValueError(\"WANDB_API_KEY가 비어 있습니다. userdata 등에 키를 설정해주세요.\")"
      ],
      "metadata": {
        "id": "ewbWeuhBhoSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key=wandb_api_key, relogin=True)"
      ],
      "metadata": {
        "id": "A-pY7zvKhwgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_SUsKqA23Gc"
      },
      "outputs": [],
      "source": [
        "#wandb 실행 시작\n",
        "if LOG_TO_WANDB:\n",
        "  wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJWQ0a3wZ0Bw"
      },
      "source": [
        "#모델 로드 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lb7M9xn46wx"
      },
      "outputs": [],
      "source": [
        "# 양자화 설정\n",
        "QUANT_4_BIT = True\n",
        "if QUANT_4_BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_O04fKxMMT-"
      },
      "outputs": [],
      "source": [
        "# tokenizer와 model 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL, trust_remote_code=True)\n",
        "\n",
        "base_model = MODEL_CLASS.from_pretrained(#멀티모달은 AutoModelForImageTextToText로 불러오기 아니면 일반 text는 AutoModelForCausalLM\n",
        "    MODEL_ID,\n",
        "    quantization_config=quant_config,\n",
        "    dtype = torch_dtype,\n",
        "    attn_implementation = \"sdpa\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DaOeBhyy9eS"
      },
      "source": [
        "#학습에 사용할 데이터 추출"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0].prompt"
      ],
      "metadata": {
        "id": "iWgZH16zleo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list = []\n",
        "val_list = []\n",
        "test_list = []\n",
        "\n",
        "\n",
        "for prompt in train_data:\n",
        "      train_list.append(prompt.prompt)\n",
        "for prompt in test_data:\n",
        "      test_list.append(prompt.prompt)\n",
        "for prompt in val_data:\n",
        "      val_list.append(prompt.prompt)"
      ],
      "metadata": {
        "id": "pNU3JdTJlEks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_list(train_list)\n",
        "eval_dataset = Dataset.from_list(val_list)\n",
        "test_dataset = Dataset.from_list(test_list)"
      ],
      "metadata": {
        "id": "qxE0sJggg4iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#관련 법조항까지 학습을 시키면 제대로된 결과가 나오지 않아서 제외시키기\n",
        "def del_law_list(target):\n",
        "  obj = str(target['messages'][2]['content'].split('결론')[1:]).strip()\n",
        "  target['messages'][2]['content'] = obj\n",
        "  return target"
      ],
      "metadata": {
        "id": "tEaEz42MwZCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[1]"
      ],
      "metadata": {
        "id": "QEg9aCjJXhxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del_law_list(test_dataset[1])"
      ],
      "metadata": {
        "id": "fhsPboK5wY3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_dataset = train_dataset.map(del_law_list)\n",
        "new_eval_dataset = eval_dataset.map(del_law_list)\n",
        "new_test_dataset = test_dataset.map(del_law_list)"
      ],
      "metadata": {
        "id": "X0-P7gN3wYu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_test_dataset[1]"
      ],
      "metadata": {
        "id": "h5UqNGpGwYeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(new_train_dataset)"
      ],
      "metadata": {
        "id": "hSxiYZWImqhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(new_train_dataset[0])"
      ],
      "metadata": {
        "id": "xOD1MBLzlq3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(new_train_dataset)"
      ],
      "metadata": {
        "id": "7XuVfc7mlsIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SFTTrainer에 사용할 하이퍼파라미터 설정"
      ],
      "metadata": {
        "id": "ncF9cvdx94Qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_parameters = SFTConfig(\n",
        "    output_dir=PROJECT_RUN_NAME,\n",
        "    num_train_epochs=5,\n",
        "    packing=False,\n",
        "\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,#너무 크게 설정하면 학습속도 느려지고, 학습 성능 안나오니 잘 조절하기\n",
        "\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "\n",
        "    learning_rate=1e-4,\n",
        "    fp16=True if torch_dtype == torch.float16 else False,\n",
        "    bf16=True if torch_dtype == torch.bfloat16 else False,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    save_safetensors=True,\n",
        "\n",
        "    logging_steps=50,\n",
        "    report_to=\"wandb\" if LOG_TO_WANDB else \"tensorboard\",\n",
        "    run_name=RUN_NAME,\n",
        "\n",
        "    push_to_hub=True,\n",
        "    hub_strategy=\"every_save\",\n",
        "    hub_model_id=HUB_MODEL_NAME,\n",
        "    hub_private_repo=True,\n",
        "\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,\n",
        "        \"append_concat_token\": True,\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    args=train_parameters,\n",
        "    train_dataset=new_train_dataset,\n",
        "    eval_dataset=new_eval_dataset,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "b7wMP41hibdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습시작\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "GeeeYdVpyImL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#마지막 모델 저장\n",
        "trainer.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
        "print(f\"Saved to the hub: {PROJECT_RUN_NAME}\")"
      ],
      "metadata": {
        "id": "oYklP2Xw8yX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32vvrYRVAUNg"
      },
      "outputs": [],
      "source": [
        "#wandb 종료\n",
        "if LOG_TO_WANDB:\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# free the memory again\n",
        "del base_model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "DIvCeIYJkP0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#모델 출력 테스트"
      ],
      "metadata": {
        "id": "Qf6SKYfS_7J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RUN_NAME = \"2025-12-04_21.54.03\"\n",
        "PROJECT_NAME = 'law_ai5'\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "REVISION = \"\" # or REVISION = None\n",
        "FINETUNED_MODEL = f\"{HF_USER}/{PROJECT_RUN_NAME}\""
      ],
      "metadata": {
        "id": "Kx7tIvFD_HkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FINETUNED_MODEL"
      ],
      "metadata": {
        "id": "6yhIkhRb_M4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uNt32X3GI_2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# Load Model base model\n",
        "model = MODEL_CLASS.from_pretrained(MODEL_ID, low_cpu_mem_usage=True)\n",
        "\n",
        "# Merge LoRA and base model and save\n",
        "peft_model = PeftModel.from_pretrained(model,FINETUNED_MODEL)\n",
        "\n",
        "\n",
        "processor = AutoTokenizer.from_pretrained(TOKENIZER_MODEL)\n",
        "processor.save_pretrained(\"merged_model\")"
      ],
      "metadata": {
        "id": "VbMLoK0LzF4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_test_dataset[13]"
      ],
      "metadata": {
        "id": "DXhlxceo_rAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = processor"
      ],
      "metadata": {
        "id": "EYk38Y6ulCZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "# Load the model and tokenizer into the pipeline\n",
        "pipe = pipeline(\"text-generation\", model=peft_model, tokenizer=tokenizer)\n",
        "\n",
        "# Load a random sample from the test dataset\n",
        "rand_idx = randint(0, len(test_dataset))\n",
        "test_sample = new_test_dataset[19]\n",
        "\n",
        "# Convert as test example into a prompt with the Gemma template\n",
        "stop_token_ids = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")]\n",
        "prompt = pipe.tokenizer.apply_chat_template(test_sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Generate our SQL query.\n",
        "outputs = pipe(prompt, max_new_tokens=300, do_sample=False, temperature=1, eos_token_id=stop_token_ids, disable_compile=True)\n",
        "\n",
        "# Extract the user query and original answer\n",
        "\n",
        "print(f\"Original Answer:\\n{test_sample['messages'][2]['content']}\")\n",
        "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
      ],
      "metadata": {
        "id": "QbMoOK2Bzuhu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}